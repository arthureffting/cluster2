import argparse
import os
from pathlib import Path

import cv2
from scripts.new.domain.training_image import TrainingImage
from scripts.new.iam_conversion.concave import run_transformation_approach
from scripts.new.iam_conversion.img_xml_pair import ImageXmlPair
from scripts.new.iam_conversion.stepper import to_steps
from scripts.new.read_conversion.read_converter import ReadConverter
from scripts.new.sampling.line_augmentations import LineAugmentation
from scripts.utils.files import create_folders, save_to_json
from scripts.utils.line_dewarper import Dewarper, Dewarper2
from scripts.utils.read_splits import Split

parser = argparse.ArgumentParser(description='Prepare data for training')
parser.add_argument('--dataset', default="iam")
args = parser.parse_args()

# Get environmental data generated by the bash script
data_folder = os.getenv("DATA_FOLDER") if os.getenv("DATA_FOLDER") else "data"
originals_folder = os.getenv("ORIGINAL_FOLDER") if os.getenv("ORIGINAL_FOLDER") else "data/original"
splits_folder = os.getenv("SPLIT_FOLDER") if os.getenv("SPLIT_FOLDER") else "data/split"

# Should store everything under data/sfr/<dataset>
target_folder = os.path.join(data_folder, "sfrs", args.dataset)
database_original_folder = os.path.join(originals_folder, args.dataset)
database_splits = os.path.join(splits_folder, args.dataset)

print("[Preparing images] Model: SFRS | Dataset: ", args.dataset)

if args.dataset == "iam":
    splits = Split.read_iam_data(database_splits)
    for dataset in splits.set_map:
        dataset_json_data_path = os.path.join(target_folder, "pages", dataset + ".json")
        dataset_json_data = []
        pairs = []
        training_indexes = set([line.page_index for line in splits.sets[dataset].items])
        for page_index in training_indexes:
            img_path = os.path.join(database_original_folder, "pages", page_index + ".png")
            xml_path = os.path.join(database_original_folder, "xml", page_index + ".xml")
            assert os.path.exists(img_path) and os.path.isfile(img_path)
            assert os.path.exists(xml_path) and os.path.isfile(xml_path)
            # index, base_folder, img_filename, xml_filename
            pair = ImageXmlPair(page_index, database_original_folder, img_path, xml_path)
            pairs.append(pair)

        pairs = pairs[0:2]

        # For each pair
        # Create a folder with its name
        # Extract lines
        # Create JSON with line information
        # Add to dataset json data
        for pair in pairs:
            folder_path = os.path.join(target_folder, "pages", "data", pair.index)
            create_folders(os.path.join(folder_path, "something.txt"))
            image_json_path = os.path.join(target_folder, "pages", "data", pair.index, pair.index + ".json")
            image_json = []
            pair.set_height_threshold(0.1)
            image_data = run_transformation_approach(pair, alpha=0.0025)
            steps = to_steps({pair.index: image_data}, [pair])
            image_steps = steps["images"][0]
            dataset_json_data.append([image_json_path, image_steps["filename"]])
            image = TrainingImage(image_steps)

            for line in image.lines:
                LineAugmentation.normalize(line)
                LineAugmentation.extend_backwards(line)
                LineAugmentation.extend(line, by=6, size_decay=0.9, confidence_decay=0.825)
                LineAugmentation.enforce_minimum_height(line, minimum_height=32)
                LineAugmentation.prevent_wrong_start(line, angle_threshold=30.0)
                line_filename = os.path.join(folder_path, pair.index + "-" + str(line.index) + ".png")
                dewarped_line = Dewarper2(line.steps).dewarped(line.masked())
                cv2.imwrite(line_filename, dewarped_line)
                image_json.append({
                    "gt": line.text,
                    "image_path": line_filename,
                    "steps": [{
                        "upper_point": [step.upper_point.x, step.upper_point.y],
                        "base_point": [step.base_point.x, step.base_point.y],
                        "lower_point": [step.lower_point.x, step.lower_point.y],
                        "stop_confidence": step.stop_confidence,
                    } for step in line.steps],
                    "sol": {
                        "x0": line.steps[1].upper_point.x,
                        "x1": line.steps[1].lower_point.x,
                        "y0": line.steps[1].lower_point.y,
                        "y1": line.steps[1].lower_point.y,
                    }
                })
            save_to_json(image_json, image_json_path)

        save_to_json(dataset_json_data, dataset_json_data_path)

    print("[Images prepared successfully]")
elif args.dataset == "read":
    print("[Preparing images for the READ dataset]")

    for folder in os.listdir(database_original_folder):
        folder_json_target_path = os.path.join(target_folder, folder + ".json")
        folder_path = os.path.join(database_original_folder, folder)
        pairs = []
        dataset_json_data = []
        for filename in os.listdir(folder_path):
            file_path = os.path.join(folder_path, filename)
            if os.path.isfile(file_path) and file_path.endswith(".png"):
                stem = Path(file_path).stem
                image_path = os.path.join(folder_path, stem + ".png")
                xml_path = os.path.join(folder_path, "page", stem + ".xml")
                pairs.append([image_path, xml_path])
        for pair in pairs:
            conversion = ReadConverter.convert(pair)

        print(pairs)

else:
    print("[Not implemented yet]")
